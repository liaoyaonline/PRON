# 关键技术
## 随机森林算法
### 决策树
决策树是一种根据对象若干属性特征来判断其所属类别的分类技术 [22] 。属性
是表示对象某一个方面的特征,一般而言,只要是对象都会拥有多个属性。不同
的是,这些属性各自的地位和重要性却各不相同。有些属性是主要属性,而有些
就显得没那么重要。在基于属性的分类过程中,不同的属性所起作用也大不相同。
对于决策树而言,如何确定对象的主要属性就显得十分重要。事实上,决策树算
法本身的实现过程就是和找出对象在分类过程中的主要属性有关 [23] 。这构成了
利用决策树实现对象分类的重要因素。为了更好的发现对象的主要属性,一般使
用信息增益的方法来找出最主要的对象属性 [24] 。这就是产生决策树根节点处对
应的属性项目的由来。而决策树 ID3 算法可以找出根节点对应的属性项目,同
时也可以进一步找出属于不同分支中的属性项目 [25] 。
图 2-1 给出了本文中使用的决策树的例子。从图中可以看出,一颗决策树会
包含多个分支,每个分支代表的是一个属性对应的测试情况。分支上的属性项目
是根据其信息增益值的大小来确定。例如,在本例中,决策树的根节点属性为
shouru,而 shouru 的取值有三种,分别是大于 6000,介于 3000 和 6000 之间,
小于等于 3000。而边表示了测试结果,叶子代表某个类别。决策树的目的就是
要预测客户属于哪个类别。在本例中,预测的是哪些客户会流失。例如,对于目
标客户 U(age=“<=30”,shouru=“>6000”,price=“高”,sex=“男”)而言,
决策树的判断结果为“未流失”。
qqqqqqqqqqqqq
决策树模型(Decision Tree)又称分类树,是成熟的统计分类工具,属于
机器学习中的一种监督学习方法。它模拟人类分类思考的模式,把样本分类的任
务,看作对“当前样本属于正类吗?”这个问题的“决策”或“判定”过程 8 。
一个决策树模型包括一个“根结点”(出发点)、多个“内部节点”(状态节
点)和多个“叶节点”
(结果节点)。叶结点是通向决策结果的节点,内部每个结
8
周志华,
《机器学习》
,清华大学,2016
点则对应于一个维度的特征,或称样本的一个属性。根节点包含样本全集。
训练决策树是为了利用有限的样本数据,得到一个正确率较高的分类器。决
策树算法的主要流程如下:
表 2.2.1 决策树算法主要流程
输入
输出
训练集数据,称为数据集,包括数据对应的类别标签
决策树
属性列表,或称特征维度,即用于内部节点分裂时的候选属性
内部节点的分裂标准
决策树算法对样本进行分类可以分为七个步骤,我们用一个流程图来说明:
图 2.2.1 决策树运行流程图
决策树的 ID3 算法是 Quinlan 于 1979 年提出的。它从信息熵的角度出发,
首先计算每个节点新增属性的信息增益,并根据其值的大小来选择分裂结点,然
后递归地构建决策树。
用信息熵来决定分裂与否,是在决策树的各级上选择用于分裂的属性。一般
来说,信息增益最大化是分裂的首选条件。
信息增益(Infomation gain)是决策树算法进行内部结点分裂最常用的分类
标准,具体计算如下:
1、计算数据集 D 的信息熵 Info(D)
K
Info(D) = − ∑
k−1
|C k |
|C k |
log 2
|D|
|D|
2、计算特征 A 对数据集 D 的条件信息熵Info A (D|)
n
Info A (D|) = − ∑
i=1
|D i |
× Info(D i )
|D|
3、计算信息增益
Gain(A) = Info(D) − Info A (D)
为了解决过拟合问题(Over Fitting),决策树通常会在模型主体迭代完成以
后进行剪枝(Pruning)。剪枝分为前剪枝和后剪枝,前剪枝是在决策树构建过程
中,提前停止构建的过程,从而达到减少树结构规模的目的,这种方法设定阈值
比较困难,所以不太常用。后剪枝是在决策树模型构建完成之后,通过删除一些
节点的方法,完成决策树规模的删减。最典型的“代价复杂性”算法是计算每个
节点剪枝后的期望错误率,如果错误率提高,就保留这个节点以下的决策树结构,
反之删除。
qqqqqqqqqqqqq
随机森林中的决策树是通过 CART 算法生成的,该算法在每个节点只考虑二
元划分,因此最终产生的决策树是一棵二叉树。但在生成随机森林的具体过程中,
训练集和候选分裂属性集的获取与传统的 CART 算法略有不同。
根据响应变量的不同,决策树的类型可分为分类树和回归树 [36] 。针对离散型
的响应变量,决策树也称为分类树,分裂规则使用的是基尼指数;针对连续型的
13华东师范大学硕士学位论文
响应变量,决策树可称为回归树,使用的分裂规则是平方误差。
基尼指数是衡量分区内数据不纯度的指标,计算公式见(2-6):
m
Gini  D   1   p i
2
(2-6)
i  1
式(2-6)中, D 是数据分区, m 是数据分区中类的个数, p i  C i , D D 是
数据分区 D 中记录属于 C i 类的概率, D 是数据分区 D 中记录的总条数, C i , D 是
数据分区 D 中属于 C i 类的记录的条数。
对于属性 A ,当 A 中有 v 个不同的值时,在不考虑全集和空集的情况下,有
 2
v

 2 2 种不同的方法可以将数据集 D 划分成两个分区 D 1 和 D 2 。对于特定的二
元划分,计算基于属性 A 的二元分裂子集的基尼指数加权和,也就是该划分的基
尼指数,计算见公式(2-7):
Gini A  D  
D 1
D
Gini  D 1  
D 2
D
Gini  D 2 
(2-7)
对于每个属性,分别计算其每一种可能的划分结果所得到的基尼指数,选择
划分后分区内纯度最大,也就是分裂子集各自基尼指数加权后总和最小的子集划
分方式作为该属性的分裂策略。
选择基于属性 A 的划分,也将导致数据集整体不纯度有所降低,其降低值的
大小计算见公式(2-8):
 Gini  A   Gini  D   Gini A  D 
(2-8)
在数据集的基尼指数固定的情况下,按属性划分后的基尼指数越小,总体基
尼指数的下降程度就越大,分裂后的集合中纯度就越高,因此,为了最大化基尼
指数的下降程度,提升数据分区内的纯度,选择基尼指数最小的属性作为该节点
的分裂属性。
分裂属性和分裂子集一起构成了 CART 分类树算法的分裂准则。
基于基尼指数的随机森林 CART 分类树算法见图 2-2。
算法:随机森林 CART 分类树算法
输入:
·数据集 D
·候选属性集合 L A
·抽取属性个数 f
输出:一棵随机森林的 CART 分类树
方法:
(1)自助法从数据集 D 中抽取训练数据 D k ;
(2)创建节点 N ;
(3)if D k 中的记录都同属于一个类 C then
(4)
返回节点 N ,标记 N 的类为 C ;
(5)for 内部节点
(6) 从 L A 中抽取 f 个属性作为候选分裂属性集合 L Ai ;
(7) 对于 L Ai 中每个属性的每种可能的划分计算其基尼指数,确定二元划分;
(8) 将 D k 划分为两部分 D k 1 和 D k 2 ;
(9) if D kj 满足停止条件 then
(10)
添加叶节点到 N ,其类别为 D kj 中的多数类;
(11) else 添加内部节点到 N ;
(12)end for
图 2-2 CART 分类树算法
回归树中衡量一个数据分区纯度的指标是平方误差,假设将数据集 D 划分为
m 个子集 D 1 , D 2 ,  , D m ,子集 D m 中的最优输出值见公式(2-9)
:
c ˆ m  ave  y i x i  D m 
(2-9)
式(2-9)中, ave   计算了子集 D m 内所有输入向量 x i 对应输出值 y i 的平均
值。
对于属性 B 进行划分,当 B 中有 v 个不同的连续值时,将 B 中的数值按升序
或降序排序后,每次将该属性可能的分裂点 s 选在两个相邻值之间,也就是中点
15华东师范大学硕士学位论文
处,这样对于属性 B 就有 v  1 种不同的方法可以将数据集 D 划分成两个子集 D 1
和 D 2 ,子集的定义见公式(2-10):



D 1  B , s   x x  B   s 和 D 2  B , s   x x  B   s

(2-10)
子集 D 1 和 D 2 划分后,子集内的平方误差见公式(2-11):
SE 1 
   y 
x i  D 1 B , s
 c 1 
2
i
和 SE 2 
   y 
x i  D 2 B , s
 c 2 
2
i
(2-11)
最优分裂属性 B 和最优分裂点 s 应该满足公式(2-12):
min   min  SE 1   min  SE 2   
B , s  c 1
c 2

(2-12)
在确定了节点处的分裂属性 B 和分裂点 s 后,划分后的子集 D 1 和子集 D 2 的
相应输出值计算见公式(2-13):
c ˆ 1  ave  y i x i  D 1  B , s   和 c ˆ 2  ave  y i x i  D 2  B , s  
(2-13)
对于每个属性的每种可能的划分计算其平方误差,选择使得划分后集合的纯
度最大,也就是拥有最小平方误差的划分点作为该属性的分裂点,对应属性作为
节点的分裂属性。
分裂属性和分裂点一起构成了 CART 回归树算法的分裂准则。
基于平方误差的随机森林 CART 回归树算法见图 2-3。
算法:随机森林 CART 回归树算法
输入:
·数据集 D
·候选属性集合 L A
·抽取属性个数 f
输出:一棵随机森林的 CART 回归树
方法:
(1)自助法从数据集 D 中抽取训练数据 D k ;
(2)创建节点 N ;
(3)if L A 为空 then
(4)
返回节点 N ,标记 N 的值为 D k 中所有输出值的均值;
(5)for 内部节点
(6) 从 L A 中抽取 f 个属性作为候选分裂属性集合 L Ai ;
(7) 对于 L Ai 中每个属性的每种可能的分裂点算其平方误差,确定二元划分;
(8) 将 D k 划分为两部分 D k 1 和 D k 2 ;
(9) if D kj 满足停止条件 then
(10)
添加叶节点到 N ,其值为 D kj 中所有输出值的均值;
(11) else 添加内部节点到 N ;
(12)end for
图 2-3 CART 回归树算法

### 随机森林介绍
随机森林(random forest)属于集成学习算法,它本质上是由多棵决策树组合
而成,可用于分类、聚类、回归等。对所有树的结果取平均可以进行回归预测(崔
东文等,2014)。其基本原理如下:如果有 M 个输入变量,在每个节点随机选择
m(m<M)个特定变量,运用这 m 个变量作为决策树分裂的候选变量,从这些
变量中选择信息含量最丰富的变量来进行节点分裂,且不对决策树进行剪枝,使
其尽可能的进行生长。最后可以通过对所有决策树做加总,预测新数据(马玥等,
2016)。预测时,如果用于分类时,多采用多数投票,而回归则经常采用平均法
计算。总体而言,随机森林具有以下优点:运算量小,但预测精度高;可以高效
的处理非线性过程;预测结果对非平衡数据和缺失数据较稳健(朱蕾等,2007)。
因此随机森林是目前性能最好的机器学习算法之一。
本文用 Python 的 Scikit-Learn 模块对随机森林进行回归建模。建模过程中需
要使用 ntree 和 ntry 两个自定义参数,以优化模型。ntree 指组成随机森林的决策
树数量,也就是重抽样次数;mtry 为所用特征变量的数目,通常是所有输入变
量的 1/3。
随机森林算法支持以下常用功能:
(1) OOB 估计:随机森林算法用 bootstrap 法进行,训练集的样本数 N 趋
向于无穷大时,各样本未被抽中的概率为(1-1/N)N,此概率值将收敛到 1/e,
约为 0.368,也就是说有 36.8%的样本被作为袋外数据(out-of-bag)。该类数据能
够估算随机森林的泛化误差。
(2) 变量重要性度量:回归变量之间存在相互作用,因此变量的重要性
较难定义。随机森林利用 OOB 来对要素的重要性测度。每棵决策树都具有一个
误分率。在随机森林算法中对袋外数据的某一变量进行随机调整顺序或者加入一
定的噪声,同时保持其它变量不变,经过修改 OOB,并由分类树计算新的预测
值,分析由袋外数据改变所引起的误差增加来估算某一变量的重要性。修改后的
OOB 误分率减去原始 OOB 误分率,再除以标准差即为变量的重要性。OOB 的
变化越显著,则该变量越重要。
(3) 随机特征选取:随机森林算法引入了随机性。它对每棵树的每个节
点的一部分变量进行了分割,树的生长只取决于所选的部分输入变量,这一做法
可解决数据高维度的问题。
qqqqqqqqqqqq
随机森林 [81-83] 是基于集成思想用于解决分类和回归问题的一种行之有效的
流行算法,属于机器学习的一个分支,近年来广泛应用于各个领域 [84, 85] 。随机森
林实质是由大量 CART(Classification And Regression Tree)决策树集合构成的,
每一颗树都是森林的基本单元,即每棵决策树都是一个估计器。在随机森林构造
中最主要的两个规则是模型训练所需的样本和特征,训练样本的选取是从观测数
据集L中采用 bootstrap 重抽样方法随机产生的,训练的特征也是从全部特征集X
中随机抽取的,即每棵决策树的训练特征是全部特征集X的子集,且此处单棵决
策树的训练与 CART 模型构建有所不同,后者是在节点分割后先剪枝再执行决
策树增长,而前者是根据随机选取的特征子集进行节点最优分割且无剪枝操作,
则森林中的每棵树都是基于特征子集的最大树。最终随机森林利用所有决策树估
计器的结果求平均值,以此作为最终的预测结果。具体计算参见公式 4-1,其中
Y i,mean 即为第i个样本的预测值, T是随机森林中决策树的数量, Y i,t,mean 是第i个样
本在第t棵树中的预测值。
1
Y i,mean = ∑ Tt=1 Y i,t,mean
T
(4-1)
模型的优势在于:首先避免了过拟合,因为每棵决策树的生长都是全部样本
和特征的一部分,且抽样方法是有放回的抽样,该种方式保证了每棵树之间是独
立的,但又是有所关联,消除了森林树间的差异性;其次,随机森林对异常值和
噪声有很高的容忍度 [86] ;再者, Bagging 思想使得模型的预测精度有很大的提高,
因为对于模型中的单棵树结果只是根据部分样本和特征做出的预测,属于弱估计
器,在预测能力上有所“缺陷”,而若将所有的弱估计器的结果进行综合,那么
预测能力会得到很大的增强;另外模型对特征会给出重要性的评价,对于了解影
响犯罪的环境因素有明显的积极作用。后续将详细介绍特征的筛选过程和对模型
的重要性。
qqqqqqqqqqqqqq
随机森林(Random Forest),于 1996 年被 Breiman 提出。随机森林最初是基
于决策树分类器的一种集成分类学习器,在后来的理论发展中逐渐引申为代表多
分类器集成学习的方法。
随机森林是 Bagging 算法在决策树分类器上的典型运用,具有耐噪声、非线
性、高准确率等优点,且不容易出现过拟合(Breiman,2001) 9 。随机森林既可以做
分类,也可以做回归。由于本文研究的是波动性分类问题,不是连续的回归问题,
所以介绍分类问题的随机森林结构。
图 2.2.2 随机森林决策结构
基于 Bagging 算法,我们可以得到 k 个原始样本集合。使用这 k 个样本集合
进行 k 轮训练,可以得到一系列的分类模型{h1(X),h2(X),⋯,h2(x)},我们把
这些分类模型称为“基分类器”。随后我们用这 k 个基分类器构成一个系统,该
系统采用投票的方式运作,最终的决策用如下公式:
k
H(x) = argmax ∑ I(h i (x) = y)
y
i=1
其中,H ( x)表示组合分类模型, h i 是第 i 个决策树,y 是第 i 个决策树的输
出,I(·)为示信函数。本式的含义是,各个决策树进行结果投票,得票最多的类
别为最终分类结果。
本章的第一节,我们已经介绍了泛化性能理论,此处我们重点介绍随机森林
9
Breiman, “Random Forests” , Machine Learning, 2001
31 / 87的泛化性能的推论。随着树的数目的增加,对于所有参数序列θ 1 , θ 2 , θ 3 ...,泛化
误差 PE 几乎处处收敛于:
P x,y (P θ (h(x, θ)) = y) − max j≠y P θ (h(x, θ)) = j < 0
上述公式说明,当森林中 Ntree 参数增加时,也即决策树基分类器的个数增
加,随机森林模型的泛化误差会随着 Ntree 收敛于一个极限值,所以不会出现过
拟合。
这个推论告诉我们一个提高随机森林泛化性能的方法:让任意两个决策树模
型相互之间的随机性上升,就可以降低这两个决策树之间的相关性。随机森林算
法就是通过 Bagging 来训练出各自带有随机性的决策树,从而在单个决策树性能
不变的情况下,抵消误差、提高预测精度。

组合分类方法是一种有助于提高模型分类准确率的技术 [1] 。其基本思想是:
一个组合分类器(ensemble)是由多个个体分类器组合而成,每个个体分类器都
有各自的分类结果,组合分类器的分类结果由个体分类器联合决定。对于分类问
题,待预测的响应变量是类别变量,组合分类器的结果由多有个体分类器投票决
定;对于回归问题,待预测的响应变量是数值变量,组合分类器的结果取个体分
类器结果的平均值。
图 2-1 随机森林原理
随机森林算法是 Breiman 在 2001 年提出的,该算法属于组合分类方法的一
种 [2][13][34] 。随机森林中的每个个体分类器都是基于 CART 算法建立的决策树,因
此随机森林算法可以解决分类和回归问题,每棵决策树的训练数据通过自助法
(bootstrap)也就是有放回的等概率随机抽样从原数据集中抽取,每个个体分类
器拥有各自不同的训练数据,所有个体分类器分类结果中的多数类或者平均值成
为随机森林的最终结果。随机森林算法原理详见图 2-1。
随机森林在生成过程中有以下特点:
(1)单棵决策树的训练样本是通过自助法得到的,也称为自助样本。自助
法,就是从原数据集中通过有放回的随机抽样的方式获取训练样本的方法,每个
样本被抽中的概率相等。由于是有放回,所以数据集中的有些记录可能被多次抽
取到训练集中,而未被抽取的记录可以放到检验集中用于估计模型的准确率。
自助法有时也称作.632 自助法,这是因为,对于拥有 d 条记录的数据集 D ,
有放回地从 D 抽取 d 次,每次抽取时, D 中每条记录被抽中的概率为 1 d ,因此
每条记录未被抽中的概率为  1 1 d  。由于是抽取 d 次,因此,某一条记录最终
一次也没被抽中的概率为  1 1 d  。当数据集 D 中记录很多, d 的值很大时,
d
 1 1 d  d 近似等于 e  1  0 . 368 。所以,由于未被抽中而放入检验集中的记录数约
占数据集 D 记录总数 d 的 36.8%,而被抽中放入训练集的记录数约占数据集 D 记
录总数 d 的 63.2%。需要注意的是,这里的 63.2%是指训练集中不同的记录是 d 的
63.2%,而训练集的数据量仍为 d 。
由于自助法的特性,会产生 36.8%的检验数据,所以随机森林在生成过程中
就可以对模型的准确率进行估计,实验证明,这种估计属于无偏估计。
(2)单棵决策树内部节点分裂时使用的候选属性集并非全属性,而是从所
有属性中随机抽取获得。CART 算法在节点分裂过程中,对于使用何种属性作为
分裂属性,使用的评价标准是基尼指数。基尼指数是基于每一个属性进行二元划
分后的结果子集进行计算的,在内部节点分裂时选择能够产生最小基尼指数的分
裂属性。随机森林中的决策树在计算基尼指数时,每一个内部节点的候选属性都
是从所有属性中随机抽取的。也就是对于数据集 D 中的 F 个属性,每次只抽取 f
个,并且 f 的值远小于 F 。在生成每棵决策树的过程中, f 的值是固定的。
(3)随机森林中的每棵决策树在生长完全后都不进行剪枝处理。传统的
CART 算法在生成决策树后一般需要进行剪枝,其使用的剪枝策略是代价复杂度,
剪枝的目的消除决策树对训练数据的过度拟合。代价复杂度是决策树中叶节点个
数与决策树错误率的函数,导致较大代价复杂度的子树会被剪枝,该方法使得模
型的结构风险最小化,在保证准确率的基础上维持了模型的简洁。但随机森林中
的决策树在生成过程中使其完全生长,并不剪枝,这是因为随机森林算法的两次
随机过程(随机抽取训练数据、随机抽取分裂属性)能够很好地避免过度拟合。
不剪枝的另一个好处就是消除了决策树的偏移。
(4)随机森林的最终结果是由所有决策树投票表决得到的,每棵决策树拥
有相同的投票权重,使用投票作为确定最终结果的策略使得随机森林算法的结果
更加稳定。对于离散型的响应变量,使用多数表决的方法,选择所有结果中得票
比例最高的作为随机森林分类的最终结果;对于连续型的响应变量,汇总所有决
策树的结果,统计其平均值作为随机森林回归的最终结果。
qqqqqqqqqqqqqqqqqq
随机森林是多个决策树的组合,每棵决策树的输出结果不尽相同,使用不同
的组合策略得到的结果也是不同的,随机森林使用的组合策略是,让模型中每棵
决策树输出的结果都具有相同的权重。也就是,对于结果为离散型的输出,随机
森林取占多数的分类结果;对于结果为连续型的输出,随机森林取所有输出结果
的平均值。
17华东师范大学硕士学位论文
随机森林算法之所以能够显著提高个体决策树的准确率,是因为复合模型降
低了个体模型的方差。
随机森林的组合算法汇总见图 2-4。
算法:随机森林组合算法
输入:
·数据集 D
·随机森林中的模型数 k
输出:随机森林模型
方法:
(1)for i = 1 to k do
(2)自助法从数据集 D 中抽取训练数据 D k ;
(3)使用 D k 和随机森林 CART 算法生成模型 M k ;
(4)end for
(5)离散数据多数表决,连续数据取均值;
图 2-4 随机森林组合算法
qqqqqqqqqqqq
随机森林是将不同的决策树组合在一起的集成学习方法,目前多应用于分类
或回归。在构成随机森林的若干决策树中,每棵树都是基于随机样本的一个独立
集合产生的。在计算机科学中,决策树是一种类似于流程图的树状结构,决策树
方法在分类、预测、规则提取等领域有着广泛应用。决策树中的每个内部结点都
表示对属性的一种测试,每个分支表示测试的过程,每个叶结点表示经过测试得
到的最终决策,从根结点到叶结点就形成了一条决策路径。随机森林的弱分类器
是 CART 树(Classification And Regression Tree),当数据集的因变量为连续性数
值时,该树算法就是一个回归树,可以用叶结点观察的均值作为预测值,解决回
归问题;当数据集的因变量为离散型数值时,该树算法就是一个分类树,可用于
解决分类问题 [69] 。
机器学习的集成学习有两种主要思想,Bagging 和 Boosting。Bagging 也叫自
举汇聚法(Bootstrap Aggregating),是一种在原始数据集上通过有放回抽样重新
选出若干个新数据集来训练学习器的集成技术,达到比单一学习器更好的学习效
果 [69] ,随机森林就是运用了 Bagging 的思想。Bagging 由 Leo Breiman [70] 于 1996
年提出,旨在提高分类和回归时算法的稳定性和准确性 [71] ,降低偏差并且有助于
避免过拟合现象。使用单棵树预测时,树对训练集中的噪声较为敏感,但当有许
多树且树与树之间并无关联时,当多棵树集成在一起“分担”训练集中的噪声时,
每一棵树受到噪声的影响就随之降低。因此,Bagging 可以有效地降低模型的方
差,并确保偏差不会增加。
Bagging 的基本思路是:当有一个大小为nn的训练集DD,使用基于 Bagging 思
想的算法在该训练集上进行分类或回归时,首先从中均匀、有放回地(使用自助
26华东师范大学硕士学位论文
抽样法)选出mm个大小为nn′的子集DD ii ,作为新的训练集。其次在这mm个训练集上
使用分类、回归等算法,则可得到mm个模型,再通过取平均值、取多数票等方法,
即 可 得 到 最 终 结 果 。 当 Bagging 应 用 于 决 策 树 算 法 时 , 给 定 训 练 集 XX =
xx 1 , xx 2 , ... , xx nn ,和目标YY = yy 1 , yy 2 , ... , yy nn ,Bagging 方法重复 B 次从训练集中有放回
地采样,在这些样本上重复训练决策树模型,在训练结束之后,对未知样本xx的
预测可以通过对xx上所有单个回归树的预测求平均来实现:
1
ff̂ = BB ∑ BBbb=1 ff bb (xx′)
(3.1)
式中,bb表示一次重复采样,xx′表示第bb次重复采样得到的训练集,ff bb (xx′)表示在
xx′训练集上训练出的回归树得到的估计值。
如果简单地在同一个数据集上训练多棵决策树会产生强相关的决策树,决策
树之间的相似性较高。Bagging 是一种通过产生不同训练集从而降低决策树之间
关联性的方法。xx′上所有单个回归树的预测的标准差可以作为预测的不确定性的
估计:
σσ = �
∑ BB (ff bb �xx ′ �−ff̂ ) 2
bb=1
BB−1
(3.2)
上文描述了 Bagging 思想运用于决策树模型的基本原理,随机森林的 Bagging
与决策树 Bagging 的不同点体现在学习过程中树的每一次划分,随机森林中子树
的每一次划分过程并未采用所有特征,而是在每次划分时选取特征集的随机子集
对划分进行决策,该方法能使随机森林中的决策树之间互不相同,促使系统多样
性得到提升。随机森林在学习过程中,不仅将 Bagging 思想运用在训练集上,还
将 Bagging 思想运用在特征集上。
qqqqqqqqq
本文选取随机森林算法进行建模。随机森林算法 [65] 的基础是决策树算法。
决策树是一种基于树结构的机器学习算法,包含朴素的“分而治之”策略。一
棵决策树分为根结点、内部节点和叶节点。根结点是决策入口,包含所有样
本,每个叶结点对应决策划分的一个结果,其他每个结点对应于某个条件的判
定规则,而从根结点到叶结点的路径就对应了一个判别决策的过程。训练决策
树就是为了求得一种最佳的属性划分原则,以期产生一棵泛化能力强,可以处
理未见示例的决策树。一般地,我们希望随着划分的深入,决策树的分支结点
所包含的样本趋向属于同一类别,即结点的“纯度”越来越高。
要定量地衡量纯度,需要用到信息论中常用的一种指标,即“信息熵”。信
息熵越低,则样本集合的纯度就越高。在决策树中,使用不同的属性进行结点
划分时会使信息熵增加或减少,这种增加或减少被称为该属性的信息增益。某
个属性的信息增益越大,则表示该属性在进行划分时的“纯度提升”越大。因
此可以通过比较信息增益来进行划分属性的选择。
另一种衡量样本集合纯度的指标是“基尼指数”。基尼指数反映了自数据集中
抽取两个样本,这两个样本类别不一样的概率。于是,在划分属性时选取使基尼
指数达到最小的属性作为最优的划分属性。
有时在决策树学习中,模型过多地创建分支可能造成结点划分过程重复,出
现“过拟合”的情况,可以通过去掉一些分支的方法降低过拟合产生的风险,这
种处理方式叫做“剪枝”。
“预剪枝”及“后剪枝”是决策树剪枝常用的两种方法。
预剪枝就是指在决策树生成的过程中考察当前结点对决策树划分性能的提升能
力,如果当前结点不能提升模型泛化性能,则停止划分并将该结点转化为叶结点。
后剪枝是指在生成了一棵完整的决策树后,自下而上地对非叶结点进行评估,考
察当前结点所在子树替换为叶结点对决策树泛化能力的提升,如果有提升则将该
子树转化为叶结点。上文简要介绍了随机森林的基础——决策树的主要原理。而随机森林的本质
其实是一种集成学习方法,就是应用集成学习的思想将多个决策树作为基本学习
器组合在一起,引入一定的随机原则,从而获得比一般决策树质量更好的学习效
果。集成学习是一种通过多个学习器结合进行学习的算法系统,定义某种结合方
式和评价机制,集成学习可以融合“同质的”或“异质的”个体学习器,获得比
单一学习器更加优异的泛化性能表现。根据个体学习器的组织方式,集成学习方
法可分为序列化方法(个体学习器强依赖、串行生成)和并行化方法(个体学习
器弱依赖、可并行生成)。序列化方法的代表是 Boosting 算法,并行化方法的代
表是 Bagging 和随机森林算法,而随机森林实际上是 Bagging 的扩展变体。随机
森林以决策树作为基学习器,通过对样本的随机选择(行采样)和属性的随机选
择(列采样),从而同时增加了个体学习器之间的差异,使得最终得到的集成学
习器的泛化性能提升更多。
将随机森林算法应用于回归问题,实际上是决策树处理连续属性的问题。
连续属性的取值数目是无限的,因此不能根据属性取值来划分结点,需要对连
续特征进行离散化。对于样本集合D的某连续属性a,假设出现了n个不同的取
值,将这些取值按照升序排列,记作{a 1 , a 2 , ... , a n },则可认为属性a的候选划分
点包含n − 1个元素,即
T a = {
a i + a i+1
| 1 ≤ i ≤ n − 1}
2
(3.5)
这样就可以像考察离散属性值一样来计算对应连续属性的信息增益:
Gain(D, a) = max Gain(D, a, t)
t∈T a
|D tλ |
= max Ent(D) − ∑
Ent(D tλ )
t∈T a
|D|
(3.6)
λε{−,+}
其中Gain(D, a, t)即样本集合D在被t二分后所得到的信息增益。因此,我们求得
令Gain(D, a, t)取得最大值的属性值作为划分点。
随机森林就是按照以上原则,应用随机的方式建立一个森林,这个森林由很
多相对独立的决策树组成,他们都是使用在数据集的行列上一定数量的有放回采
样基础上独立训练而成的基学习器。每个决策树都可以被看作对目标问题的某个
22华东师范大学硕士学位论文
角度的解释,而对所有决策树得到的回归结果进行平均,就是随机森林回归的结
果。基于随机森林的随机性和决策树的独立性,该算法在处理特征维数较多的情
况下也非常有效,并且并行效率非常高,也不容易出现过拟合现象。
### 随机森林算法性质
泛化能力是衡量模型性能的重要指标。泛化能力指的是在未经训练的数据上
模型所表现出的性能,模型的泛化能力越强,说明其在未知数据上的预测能力越
强。事实上,可以得到一个在训练数据集 100%准确率的模型,但一般情况下,
这个模型用在新的数据上时准确率会表现得较低,也就是泛化能力较弱。当数据
量较大时,模型的泛化能力可以用模型在检验数据上的误差水平表示;在检验数
据并不充分的情况下,可以理论推导出随机森林模型的泛化误差概率上界。
假设给定一个随机森林的组合分类器表示为  h 1  X  , h 2  X  ,  , h k  X   , h  X  是
单个分类器对于输入向量 X 所产生的输出结果。对于通过自助法随机抽取的训
练集向量 X 、 Y , Y 是训练集中 X 的对应分类结果,定义间隔函数(margin function)
见公式(2-1):
mg  X , Y   av k I  h k  X   Y   max av k I  h k  X   j 
j  Y
(2-1)
式(2-1)中, I   为指示函数。间隔函数衡量了平均正确分类数超过平均
错误分类数的程度。间隔函数越大,说明模型分类的置信水平越高。
因此,随机森林的泛化误差可以定义为公式(2-2):
11华东师范大学硕士学位论文
PE *  P X , Y ( mg ( X , Y )  0 )
(2-2)
对于随机森林模型, h k  X   h  X ,  k  ,  是单棵决策树独立同分布的参数
向量。当随机森林的规模很大时,其性能遵循大数定律规律,即随着树的棵数的
增加,几乎可以肯定,所有序列  1 ,  , PE * 收敛于公式(2-3):
P X , Y ( P  ( h ( X ,  )  Y )  max P  ( h ( X ,  )  j )  0 )
j  Y
(2-3)
这就说明,随着随机森林中树的棵数的增加,模型不会陷入过度拟合 [31] ,其
泛化误差有一个理论上的极限值。
另外,随机森林算法还具有以下优点 [2] :
(1)随机森林算法的性能可以和 Adaboost 算法媲美,有时甚至表现得更好;
(2)随机森林对噪声点和离群值更加鲁棒;
(3)由于内部节点分裂时只考虑很少的属性,随机森林相较于装袋(bagging)
或提升(boosting)计算更快;
(4)随机森林可以在不需要额外数据的情况下给出错误率、相关性、变量
重要性的估计;
(5)随机森林算法易于实现且有利于并行化计算。
qqqqqqqqqqqq
通过对随机森林原理的梳理和分析,简述随机森林的优缺点以及在实际应用
中需要注意的地方。总体而言,随机森林模型的并行化结构使得训练速度很快,
对于大样本训练具有优势。由于在子树结点进行划分时可以随机地选取划分特征,
当模型特征集维度较高时,仍可高效地进行模型训练。基于 Bagging 思想,随机
29华东师范大学硕士学位论文
森林模型方差较小、泛化能力强。随机森林在具有上述优势的同时,随机森林在
噪音干扰过大的样本中会发生过拟合现象。
### OOB重要性
在大数据背景下,各式各样数据之间的关联越来越复杂化和多样化,一种事
件或现象的出现背后会隐藏着各种各样的关联因子,如何识别和鉴定出关联性较
强的因子是解决问题的关键前提。因此,在对相关问题进行如解释或预测等定量
化研究的时候,特别当研究数据集维度很高的时候,特征选择就变得极为重要,
一般要解决的问题是将不相关或冗余变量进行剔除。其优点是降低了冗余或不可
信变量对模型性能的影响,同时计算成本也会随着变量的减少而降低,最重要的
是使得模型的可理解性得到大幅提高。
特征重要性的选取依赖于袋外数据(Out-Of-Bag, OOB)。随机森林在模型训
练时通过自助法(Bootstrap)从训练集中有放回地抽取k个样本集,对该k个样本
集分别进行训练即可得到k个估计器,综合所有的模型估计结果即可得到最终结
果。因此,在该样本集抽取过程中,每次的抽取过程只会随机抽取约 1/3 的小部
分样本,剩下未被抽中的样本数据即为 OOB,并以此作为模型精度的检验集,
故这种使用袋外数据估计模型精度的方法称为 OOB 估计 [87] 。该方法的优势就是
可以直接获得模型的泛化误差,而不需要通过交叉验证或额外的计算来检验模型
的优劣,且 OOB 估计属于无偏估计 [88] 。
在随机森林的特征选择理论中,特征重要性(Variable Importance, VI)是一
个十分重要的指标,反映了该特征对模型预测能力的贡献度,同时也在一定层面
上表现了对模型的解释能力。其原理是在通过迭代训练森林中单棵树t的过程中
随机置换第i个分裂特征,然后比较该特征被置换前后估计器的 OOB 误差变化,
如果该特征在被置换后,模型的精度大大降低,则说明该特征与响应变量有较强
的关联性,并将变量X i 被置换前后的模型精度的差值作为衡量该特征对所在估计
器的重要程度,记作VI ti (当特征i没有出现在第t棵树中,VI ti = 0),因此,特征
i对随机森林中的每一棵树都有一个重要性值。对于整个模型而言,特征i的重要
性VI ti 是森林中所有树对应的特征i重要性的平均值,计算公式如下:
VI i =
1
ntree
∑ ntree
t=1 ( EP ti − E ti )
(4-2)
其中ntree代表随机森林中决策树的数量, E ti 表示将变量X i 置换之前第t棵树
的 OOB 误差,EP ti 表示将变量X i 置换之后第t棵树的 OOB 误差。
以上是基于 OOB 估计得到特征相对重要性的基本原理,在实际运用中,R
语言的 RandomForest 模块会给出两个指标反映特征的重要程度,分别为均方误
差增加值(IncMSE)和节点纯度增加值(IncNodePurity),因此本文同时选取
这两个指标来进行特征筛选,此举相比较用单一的指标优势在于使得模型更加健
壮 [89] 。
qqqqqqqqqqqqqqqq
随机森林可以返回多种衡量变量重要性的度量,其中最可靠的是基于树节点
分裂属性被随机替换后的分类准确度降低值 [19] 。为了筛选变量,需要迭代拟合随
机森林,在每次迭代过程中生成一个新的森林,这些森林是由剔除最不重要变量
后的变量集合生成的,最终选择的是产生最小 OOB 误差的变量集合。
OOB 误差通过 OOB 估计(Out-of-Bag Estimation)获得 [25] 。对于随机森林
中的每个个体分类器来说,每次选择用于学习的训练数据都是通过自助法从原数
据集中有放回地等概率抽取的自助样本,原数据集中约 37%的数据在自主样本之
外,这部分数据被称为袋外(Out-of-Bag)数据。利用这些数据可以估计每个个
体分类器的各种统计量,如泛化误差、强度、相关性等,随机森林的 OOB 误差
是每个个体分类器的 OOB 误差估计值平均后得到的 [8] 。
12第二章 随机森林研究方法
基于 OOB 误差估计的变量选择,其基本原理是:在每次迭代的过程中,通
过预测变量 X j 的随机置换,该预测变量与响应变量 Y 的关联被打破 [7] 。当被置换
后的预测变量 X j 连同其他未被置换的预测变量被用于预测袋外数据的响应变量
时,预测准确度会发生变化,若准确度发生较大幅度的下降,说明置换前的预测
变量 X j 与响应变量有关,该变量的重要性较大。
因此,可以利用预测变量 X j 随机置换前后准确度变化的差值,所有决策树
平均后作为衡量变量重要性的度量,计算过程如下:
首先,计算预测变量 X j 在第 t 棵树中的重要性,计算见公式(2-4):
( t )
VI ( X j

) 

I y i  y i
i  B ( t )
B
( t )
( t )
  
i  B ( t )
式(2-4)中, B (t ) 是第 t 棵树的 OOB 数据, y ˆ i
替换前第 i 条记录的预测结果, y ˆ i ,  j
( t )

I y i  y i ,  j
B
( t )
( t )

( t )
(2-4)
 f ( t ) ( x i ) 是预测变量 X j 被
 f ( t ) ( x i ,  j ) 是预测变量 X j 被替换后第 i 条记
录的预测结果。由于节点的候选分裂属性是随机抽取的,因此会出现第 t 棵树中
没有预测变量 X j 的情况,此时定义 VI ( t ) ( X j )  0 。
变量 X j 的重要性计算结果是取所有树中该变量重要性的平均值,计算见公
式(2-5):

VI  X  
j
ntree
t  1
VI  t   X j 
ntree
(2-5)
最终选择变量数目最少且导致最小误差率的变量集合。
## 面向对象和UML建模
### 面向对象开发的基本特征
面向对象方法(Object-Oriented Method)是现在很成熟的一种软件开发方法,
它借助于对象和类的概念对系统中的实体及其行为进行描述 [14] 。对象由多个属
性和方法组成,在面向对象开发过程中,可以把所有的实体当作对象来看待。而
类是对所有具有相同属性和方法的对象的总称或抽象。面向对象开发方法比传统
的面向过程的方法更具有优势,具体表现在代码的安全和复用上。因为对象的方
法是封装了一组代码的行为,对外部调用者而言,是不能看到对象内部的结构,
除了那些可以被共享的公开信息。对于代码的复用问题,这在软件开发过程中也
是很重要的一个问题。
从对象的实现方式来看,由父类派生的子类所产生的对象会继承父类中的对
象的特征,包括用于控制行为的操作代码 [15] 。这本身就是一种很好的代码复用
的情况。通常对于系统中的功能实现,会通过定义类的方法去完成。当面对具有
相似功能的时候,无需定义新的类,而只需要创建这个类的实例就可以去完成这
个功能。如果涉及到参数的变化,则可以通过创建这个类的子类去完成。而这里
面都包含了代码复用的过程,这对于维护系统的模块化和紧凑性是十分有用的。
另外,面向对象开发还具有开发效率高,开发周期短的特点。而且也便于开展对
系统进行功能扩充和维护工作。
qqqqqqqq
面向对象方法(Object-Oriented Method)是现在很成熟的一种软件开发方法,
它借助于对象和类的概念对系统中的实体及其行为进行描述 [29] 。对象由多个属
性和方法组成,在面向对象开发过程中,可以把所有的实体当作对象来看待。而
类是对所有具有相同属性和方法的对象的总称或抽象。面向对象开发方法比传统
的面向过程的方法更具有优势,具体表现在代码的安全和复用上。因为对象的方
法是封装了一组代码的行为,对外部调用者而言,是不能看到对象内部的结构,
除了那些可以被共享的公开信息。对于代码的复用问题,这在软件开发过程中也
是很重要的一个问题。
从对象的实现方式来看,由父类派生的子类所产生的对象会继承父类中的对
象的特征,包括用于控制行为的操作代码 [30] 。这本身就是一种很好的代码复用
的情况。通常对于系统中的功能实现,会通过定义类的方法去完成。当面对具有
相似功能的时候,无需定义新的类,而只需要创建这个类的实例就可以去完成这
个功能。如果涉及到参数的变化,则可以通过创建这个类的子类去完成。而这里
面都包含了代码复用的过程,这对于维护系统的模块化和紧凑性是十分有用的。
另外,面向对象开发还具有开发效率高,开发周期短的特点。而且也便于开展对
系统进行功能扩充和维护工作。
### 面向对象中的 UML 建模
UML 是一种通用的、可视化的统一建模语言 [16] 。对软件系统进行建模是一
种必须要掌握的基本技能。尤其是在面向对象开发时代,一个好的建模过程可以
大大降低系统的开发难度和复杂性。在传统的面向过程开发中,通常使用的是瀑
布模型。而在面向对象开发中,主要使用 Rational 统一过程(RUP)方法。在
RUP 方法使用过程中,有一个很重要的特征就是需要在软件的各个开发阶段建
立合适的开发模型 [17] 。由于软件的开发过程涉及到需求分析、系统设计、代码
编写等诸多过程,因此,使用 RUP 方法都需要为其建立相应的模型。UML 在建模方面可以说是博采众长,它集中了许多建模工具的优点,把建模工作的可视化
和文档化体现得淋漓尽致。一方面,可视化的建模效果可以帮助软件设计人员更
好的把握系统的总体特征;另一方面,文档化也可以帮助软件开发人员便于阅读
和保存建模结果,以便今后的重复利用 [18] 。目前,把 RUP 和 UML 建模相结合
的软件开发模式已经成为一种主流面向对象的开发方式。UML 建模的可视化主
要体现在各种建模图形上 [19] 。常用的 UML 建模图形包括用例图、活动图、类图、
时序图、部署图和包图等。不同的图形被使用在不同的开发阶段中。例如,在系
统需求分析时期,可以使用用例图和活动图来描述系统功能及其主要的业务活动
流程 [20] 。在系统设计阶段,可以使用类图和时序图来描述软件系统的静态结构
和动态模型。因为最终系统功能的实现需要借助软件的作用,而对软件结构建模
可以提高软件的编写效率,降低软件开发的难度 [21] 。在系统实现阶段,部署图
和包图具有很大的应用价值,因为系统整体组织结构上需要有软件和硬件的相互
配合,而部署图和包图就可以体现出这种软硬件之间的逻辑关系 [22] 。总之, UML
建模可以很好的体现系统开发过程中所涉及的系统静态结构和动态特征。
qqqqqqqqq
UML 是一种通用的、可视化的统一建模语言 [22] 。对软件系统进行建模是一种
必须要掌握的基本技能。特别是在使用 JAVA 开发的时候,如果能有一个比较好
的建模的过程,那么在进行系统开发的过程中,你会发现系统的开发难度以及复
杂性都会得到显著的降低。在以前的的开发中,通常使用瀑布模型。在 JAVA 的
开发中,使用到了 Rational 统一过程(RUP)方法。在使用 RUP 方法的过程中,在
系统研发的各个过程阶段都需要进行建立一些适当的开发模型 [23] 。由于软件开
发过程涉及许多过程,如需求分析、系统设计、代码编写等,因此有必要为 RUP
方法构建相应的模型 [24] 。目前,把 RUP 和 UML 建模相结合的软件开发模式已经
成为一种主流面向对象的开发方式。UML 建模的可视化主要体现在各种建模图形
上 [25] 。常用的 UML 建模图形包括用例图、活动图、类图、时序图、部署图和包
图等。不同的图形被使用在不同的开发阶段中。例如,在系统需求分析时期,可
以使用用例图和活动图来描述系统功能及其主要的业务活动流程 [26] 。在系统设
计阶段,可以使用类图和时序图来描述软件系统的静态结构和动态模型。因为最
终系统功能的实现需要借助软件的作用,而对软件结构建模可以提高软件的编写
效率,降低软件开发的难度。在系统实现阶段,部署图和包图具有很大的应用价
值,因为系统整体组织结构上需要有软件和硬件的相互配合,而部署图和包图就
第 10 页基于微信小程序的在线报名管理系统的设计与实现
可以体现出这种软硬件之间的逻辑关系 [27] 。总之,UML 建模可以很好的体现系统
开发过程中所涉及的系统静态结构和动态特征。
qqqqqqq
UML,其实就是指统一建模语言,是一种编制软件规划的模型语言 [2] 。它可以对复杂
软件系统的各个部分实现可视化,同时构建软件模型以及系统文档。
UML 中一共定义了 9 种基本图,在系统开发的过程中,不同的阶段会使用不同的图
来辅助描述。一般在系统分析时会使用到用例图,在系统设计时会使用到时序图,在系
统实现时则会使用到类图和对象图,其他的图则不常用。
## MySql数据库
经过选型和比较,学生成绩分析与预警系统在数据库存储上选定的是微软公
司的 Microsoft SQL Server 2014,在这个数据产品中可以满足用户的业务和性能
需求,可以按照用户的需求进行数据对象的存储,在结构化数据中处于相对领先
的地位 [41] 。对于数据库的产品来说,不仅仅是考虑数据容量的问题,还需要考
虑的是数据存储的成熟度问题。通过对各类的数据库产品的比较,本文选定的
Microsoft SQL Server 2014 这个数据库可以满足需要 [42] 。
随着信息技术发展,目前的数据类型变化多端,对于数据库产品来说,也需
要跟进,使之能够适应这样的变化,可以为用户提供良好的数据存储的服务,并
对海量的数据对象进行安全稳定的存储 [43] 。对于数据存储来说,一方面是存储
容量,另一方面是需要有较好的管理便利性。目前,数据库产品在经过几十年发
展之后,能够适应现代社会对数据存管理的需要,能够完成良好的数据传送的要
求 [44] 。
此外,数据库管理需要有统一标准规范的语言,才能够完成数据对象的设计
和存储,完成各类的数据对象的操作,通过标准规范的数据管理语言和操作,完
成数据对象的各类需求。目前这个数据产品管理系统可以很好地满足用户的需求,
可以对数据对象提供增加、删除、修改和更新的服务 [45] 。
在 Microsoft SQL Server 2014 数据库管理系统中,具有如下的优势与特性,
有如下几个方面 [46] :
首先,它可以很好地满足数据的移植性能,可以在不同的平台中运行,代码
经过良好的测试,具有安全稳定的运行效率 [47] 。
其次,它具有较好的适应性。在不同的操作系统中可以很好地运行,可以支
持不同的用户语言的习惯,满足多语言的接口。可以在源代码中实现数据对象的
高效查询,满足极短时间范围内的数据对象的查询 [48] 。
8基于挖掘算法的学生成绩分析与预警系统的设计与实现
最后,数据访问方式便捷,数据连接方式提供多种工具实现。可以有很好地
提高数据管理效率,能够支持数据存储引擎,满足海量数据存储需要 [49] 。
综上所述,经过分析比较,Microsoft SQL Server 2014 与其他的数据库产品
和系统具有一定的特性和优势,可以满足用户的数据存储和管理的要求
qqqqqqqqqqqqqqqqqqqqq
数据库管理系统由一组相互关联的数据和一组用于访问这些数据的程序组
成。这组数据通常称为数据库,其中包含了某个企业的信息。DBMS 的主要目标
是要提供一个可以方便、高效地存取数据信息的环境 [20] 。
2.4.1 MySQL 介绍
MySQL 把用户需要存储的数据放在不同的数据表中,并不是把所有数据一起
放在一个表中。这样做的好处就是提高了数据处理的速度和灵活性 [21] 。
MySQL 也是采用的 SQL 语言,用于访问和处理数据库的标准的计算机语言。
MySQL 有两个版本分为社区版、商业版。同时也采用了双重授权策略。之所以很
多中小型企业选择 MySQL 作为他们首选的数据库,主要是因为:体积小、速度快、
成本低、开源等特点。
MySQL 通过 HandlerSocket 插件提供了 API 访问接口。HandlerSocket 提供
类似于 nosql 的网络服务,以守护进程插件的形式提供服务。它不会对数据进行
直接的处理,而是先侦听配置的端口,使用 NoSQL/API 接收通信协议,然后调用
存储引擎,通过 MySQL 中的处理程序 API 处理数据。
2.4.2 主从技术复制原理
Slave 的 IO 线程连接上 Master,通知 Master 需要读取指定存放日志文件
的指定位置之后的内容,并处于侦听状态,等待 Master 的 Binlog Dump 发送更
新。
第 9 页基于微信小程序的在线报名管理系统的设计与实现
Master 接收到来自 Slave 的 IO 线程的请求后,若发现所请求的文件和位置
之后有更新,则把更新内容返回给 Slave 的 IO 线程。返回的信息还包括主服务
端 bin-log 文件的名称及其在 bin-log 中的位置。
从服务器的 IO 线程接收信息,依次将接收到的日志内容写到从服务器端
RelayLog 文件的末尾,并将主服务器端 bin-log 的文件名和位置记录到
Master.info 文件中。能够在一次读取时清楚地告诉 Master 需要哪个 Bin-log
post-log。
Slave 的 SQL 线程检测到中继日志中的新内容时,当日志文件的内容在主服
务端实际执行时,它会立即将其解析为可执行查询语句,并自己执行这些查询。
通过这种方式,实际上在主服务端和从服务端执行相同的查询,因此两端的数据
完全相同。
## B\S架构
B/S,B 代表的是 Browser,即浏览器;S 代表的是 Server,即服务器 。B/S 架构,
其实是一种特殊的 C/S 架构,只不过这个 C(Client)特指浏览器。
对于 B/S 架构的系统,客户端计算机只需要安装常用的浏览器或者直接用系统自动
安装的 IE 浏览器就可以访问服务端。浏览器只负责显示用户页面和提交请求,所有的
逻辑业务都集中在服务器中处理。这样的优势在于系统升级维护时,只需要将服务器端
的程序代码进行升级即可,而不需要对客户端进行处理,如此极大地减轻了升级所需的
工作量,降低了人工成本。
qqq
B/S 架构是目前较为流行的用户访问系统的软件开发模式。因为这个架构是
基于浏览器的,软件研发主要关注后台的业务处理逻辑,通过三层架构可以将数
据与表现分离出来。前端是客户端,主要是用户的浏览器 [38] 。用户只需使用自
己设备中的浏览器进行访问,不用安装部署任何软件,后期的使用更没有升级维
护软件的麻烦。中间是服务器端,是进行业务处理的。后台是数据库,详细的架
构具体如图 2-2 所示 [39] 。
客户端
(Internet用户)
客户端
(操作人员)
服务器
(Web Server)
数据库
(Database)
图 2-2 B/S 架构图
在上述的 B/S 架构中主要包括了三个部分,分别是客户端、服务器、数据库:
7基于挖掘算法的学生成绩分析与预警系统的设计与实现
(1)客户端:主要的工作是接收用户的数据信息。由用户端的浏览器工具
来完成,显示系统所处理结果。用户端在浏览器中通过 HTTP 请求将数据发送给
服务器端,对于符合要求的请求在服务器端进行处理,将结果显示在用户端中。
(2)服务器端:主要是进行业务的逻辑处理,在三层架构中起到上下承启
作用。
(3)数据库:主要是进行中间业务的请求,对各类请求的响应和处理,并
将处理结果的数据完成存储 [40] 。
综上所述,在本系统的开发中,主要工作是中间业务的处理和数据库设计,
充分利用三层架构的优势,建立业务与数据之间的连接。
